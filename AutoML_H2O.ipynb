{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef5ac3b",
   "metadata": {},
   "source": [
    "# H2O practical approach\n",
    "## The following code can be used in an Jupyter Notebook (Python 3.8.X, H2O cluster version 3.36.0.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b347d6a",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a23b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a0d5a",
   "metadata": {},
   "source": [
    "Attempting to start a local H2O server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99365901",
   "metadata": {},
   "outputs": [],
   "source": [
    "Read the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MOVIESTREAM_CHURN_RED_TRAIN.csv'\n",
    "dataframe = read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353e47f",
   "metadata": {},
   "source": [
    "Data preprocessing: filling missing values, substitution of values, select the training features and target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['YRS_CURRENT_EMPLOYER'] = dataframe['YRS_CURRENT_EMPLOYER'].fillna(0)\n",
    "dataframe['IS_CHURNER'] = dataframe['IS_CHURNER'].replace(['no'], 0)\n",
    "dataframe['IS_CHURNER'] = dataframe['IS_CHURNER'].replace(['yes'], 1)\n",
    "array = dataframe.values\n",
    "ID_train = array[:,0]\n",
    "y_train = array[:,-1]\n",
    "htrain = h2o.H2OFrame(dataframe)\n",
    "htrain['IS_CHURNER'] = htrain['IS_CHURNER'].asfactor()\n",
    "x = htrain.columns\n",
    "y = 'IS_CHURNER'\n",
    "x.remove(y)\n",
    "x.remove('CUST_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c997685",
   "metadata": {},
   "source": [
    "__Model selection and tuning__. The time limit for running AutoML is set to five minutes. In this scenario we removed algorithms like Stacked Ensemble and Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = H2OAutoML(max_models = 3, max_runtime_secs=300, exclude_algos=['StackedEnsemble','DeepLearning'], seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b4cf5",
   "metadata": {},
   "source": [
    "__Training H2O AutoML__. The AutoML leaderboard uses cross-validation metrics to rank the models. The leader model is stored at _aml.leader_ and the leaderboard is stored at _aml.leaderboard_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.train(x=x, y=y, training_frame=htrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e10d40",
   "metadata": {},
   "source": [
    "Checking the Leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb32362",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "lb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284f802",
   "metadata": {},
   "source": [
    "__Save the best model to filesystem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dded56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(aml.leader, path = \"h2o_model\")\n",
    "print(model_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ee16d",
   "metadata": {},
   "source": [
    "__H2O Explainability interface__ is a convenient wrapper to a number of explainabilty methods and visualizations in H2O. The _explain()_ function generates a list of explanations â€“ individual units of explanation such as a Partial Dependence plot, a Feature Importance plot or a SHapley Additive exPlanations (SHAP) Summary of Top Tree-based Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "xplain_model = aml.leader.explain(htrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cfa4b",
   "metadata": {},
   "source": [
    "__Predicting on train data using the leader model__. The predict function outputs predicted classes, as well as the probability estimates for each of the classes (confidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1632fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_h2o = aml.leader.predict(htrain)\n",
    "pred_pandas=pred_h2o.as_data_frame(use_pandas=True)\n",
    "probs = pred_pandas.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11304232",
   "metadata": {},
   "source": [
    "__Restore the model from the filesystem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = h2o.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca95bc",
   "metadata": {},
   "source": [
    "Read the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MOVIESTREAM_CHURN_RED_TEST.csv'\n",
    "dataframe = read_csv(filename)\n",
    "dataframe['YRS_CURRENT_EMPLOYER'] = dataframe['YRS_CURRENT_EMPLOYER'].fillna(0)\n",
    "dataframe['IS_CHURNER'] = dataframe['IS_CHURNER'].replace(['no'], 0)\n",
    "dataframe['IS_CHURNER'] = dataframe['IS_CHURNER'].replace(['yes'], 1)\n",
    "array = dataframe.values\n",
    "ID_test = array[:,0]\n",
    "y_test = array[:,-1]\n",
    "htest = h2o.H2OFrame(dataframe)\n",
    "htest['IS_CHURNER'] = htest['IS_CHURNER'].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927adf57",
   "metadata": {},
   "source": [
    "__Predicting on test data using the saved model__. The predict function outputs predicted classes, as well as the probability estimates for each of the classes (confidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80690b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_h2o = saved_model.predict(htest)\n",
    "pred_pandas=pred_h2o.as_data_frame(use_pandas=True)\n",
    "probs = pred_pandas.values\n",
    "\n",
    "pred_confidence = []\n",
    "y_pred = []\n",
    "for i in range(len(probs)):\n",
    "  y_pred.append(probs[i, 0])\n",
    "  if (probs[i, 0]==0):\n",
    "    pred_confidence.append(probs[i, 1])\n",
    "  else:\n",
    "    pred_confidence.append(probs[i, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds_id = pd.DataFrame(ID_test, columns = [\"ID\"])\n",
    "ds_actual = pd.DataFrame(y_test, columns = [\"ACTUALVALUE\"])\n",
    "ds_pred = pd.DataFrame(y_pred, columns = [\"PREDICTEDVALUE\"])\n",
    "ds_prob = pd.DataFrame(pred_confidence, columns = [\"PREDICTIONCONFIDENCE\"])\n",
    "dataframe = pd.concat([ds_id, ds_actual, ds_pred, ds_prob], axis=1)\n",
    "dataframe.to_csv('h2o_test_pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242dc9f",
   "metadata": {},
   "source": [
    "__Build the confusion matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7500fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab87d7",
   "metadata": {},
   "source": [
    "__Display the confusion matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    " \n",
    "fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(2, 2), cmap=plt.cm.Greens)\n",
    "plt.xlabel('Predictions', fontsize=11)\n",
    "plt.ylabel('Actuals', fontsize=11)\n",
    "plt.title('Confusion Matrix', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc2f2b",
   "metadata": {},
   "source": [
    "__Calculate the performance metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d820596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
